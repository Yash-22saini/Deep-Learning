{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "189f4cb4",
   "metadata": {},
   "source": [
    "# Transformer Interview Notes (with Working & Mathematical Expressions)\n",
    "\n",
    "## 1. Transformer Architecture Overview\n",
    "A Transformer consists of an **Encoder-Decoder structure** where:\n",
    "- Encoder: Processes the input sequence into contextual embeddings.\n",
    "- Decoder: Uses encoder output and previously generated tokens to predict the next token.\n",
    "\n",
    "Uses **self-attention** mechanism for sequence processing.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Self-Attention Formula\n",
    "\\[ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V \\]\n",
    "\n",
    "Where:\n",
    "- Q = Query matrix\n",
    "- K = Key matrix\n",
    "- V = Value matrix\n",
    "- d_k = dimension of keys\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Query, Key, and Value Computation\n",
    "\\[ Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V \\]\n",
    "\n",
    "Each token is transformed using trainable weight matrices.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Multi-Head Attention\n",
    "\\[ MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W_O \\]\n",
    "\\[ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V) \\]\n",
    "\n",
    "Captures diverse relationships through multiple heads.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Feed Forward Network (FFN)\n",
    "\\[ FFN(x) = ReLU(xW_1 + b_1)W_2 + b_2 \\]\n",
    "\n",
    "Adds non-linearity and depth to each token representation.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Positional Encoding\n",
    "Since Transformers lack recurrence, positional encoding injects order information:\n",
    "\n",
    "\\[\n",
    "PE_{(pos, 2i)} = \\sin(\\frac{pos}{10000^{2i/d_{model}}}) \\\\\n",
    "PE_{(pos, 2i+1)} = \\cos(\\frac{pos}{10000^{2i/d_{model}}})\n",
    "\\]\n",
    "\n",
    "Added to token embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Encoder Layer Computation\n",
    "\\[\n",
    "Z' = LayerNorm(X + MultiHeadAttention(X)) \\\\\n",
    "Z = LayerNorm(Z' + FFN(Z'))\n",
    "\\]\n",
    "\n",
    "Residual connections + normalization for stability.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Masking\n",
    "Decoder uses **causal masking**:\n",
    "\\[\n",
    "Mask(i, j) =\n",
    "\\begin{cases}\n",
    "0, & j \\le i \\\\\n",
    "-\\infty, & j > i\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "Ensures no future token is seen.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Cross-Attention (Decoder)\n",
    "\\[\n",
    "Attention(Q_{dec}, K_{enc}, V_{enc}) = softmax(\\frac{Q_{dec}K_{enc}^T}{\\sqrt{d_k}})V_{enc}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Output Probability\n",
    "\\[ P(y_t | y_{<t}, X) = softmax(W_o h_t) \\]\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Loss Function (Cross-Entropy)\n",
    "\\[\n",
    "\\mathcal{L} = -\\sum_t y_t \\log(\\hat{y_t})\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Layer Normalization\n",
    "\\[\n",
    "LayerNorm(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 13. Computational Complexity\n",
    "\\[ O(n^2 \\cdot d) \\]\n",
    "\n",
    "Self-attention requires pairwise interactions between tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## 14. Training Objectives\n",
    "- **BERT (Masked LM):**\n",
    "  \\[ \\mathcal{L}_{MLM} = -\\sum_{i \\in M} \\log P(x_i | X_{\\backslash M}) \\]\n",
    "- **GPT (Causal LM):**\n",
    "  \\[ \\mathcal{L}_{CLM} = -\\sum_t \\log P(x_t | x_{<t}) \\]\n",
    "\n",
    "---\n",
    "\n",
    "## 15. Vision Transformer (ViT)\n",
    "- Divide image into patches.\n",
    "- Flatten and linearly project each patch: \\( x_i = W_e \\cdot Flatten(patch_i) \\)\n",
    "- Add positional encoding and feed to Transformer Encoder.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "| Concept | Key Formula |\n",
    "|----------|--------------|\n",
    "| Attention | softmax(QKᵀ / √dₖ) V |\n",
    "| Multi-Head | Concat(heads)Wₒ |\n",
    "| FFN | ReLU(xW₁ + b₁)W₂ + b₂ |\n",
    "| Positional Encoding | sin/cos functions |\n",
    "| LayerNorm | (x - μ)/√(σ²+ε) * γ + β |\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notes – Transformer Working & Formulas**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
