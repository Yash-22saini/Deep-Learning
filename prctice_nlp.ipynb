{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5s9HXPztRF4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd2e474f-fb9a-441f-bb80-b4a29a929b72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:\n",
            " \n",
            "Hey Yash üòÉ! Kya haal hai? Long time no see‚Ä¶ \n",
            "Kal raat I watched Avengers: Endgame üé¨üî• ‚Äì mast movie thi! \n",
            "BTW, Elon Musk üöÄ announced a new Tesla model @ California (2025). \n",
            "Main soch raha tha ki hum log ‚òï coffee pe milte jab bhi free ho‚Ä¶ \n",
            "Lemme know, ok? üëç #FriendsForever ‚ù§Ô∏è\n",
            "\n",
            "\n",
            "Cleaned:\n",
            " hey yash kya haal hai long time no see kal raat i watched avengers endgame mast movie thi btw elon musk announced a new tesla model california 2025 main soch raha tha ki hum log coffee pe milte jab bhi free ho lemme know ok friendsforever\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # 1. Remove emojis (Unicode ranges for emoticons, symbols, pictographs, flags etc.)\n",
        "    text = re.sub(r'[\\U00010000-\\U0010ffff]', '', text)\n",
        "\n",
        "    # 2. Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 3. Remove special characters (keep letters, numbers, spaces)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    # 4. Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example paragraph with emojis + Hinglish + special chars\n",
        "paragraph = \"\"\"\n",
        "Hey Yash üòÉ! Kya haal hai? Long time no see‚Ä¶\n",
        "Kal raat I watched Avengers: Endgame üé¨üî• ‚Äì mast movie thi!\n",
        "BTW, Elon Musk üöÄ announced a new Tesla model @ California (2025).\n",
        "Main soch raha tha ki hum log ‚òï coffee pe milte jab bhi free ho‚Ä¶\n",
        "Lemme know, ok? üëç #FriendsForever ‚ù§Ô∏è\n",
        "\"\"\"\n",
        "\n",
        "cleaned_text = preprocess_text(paragraph)\n",
        "print(\"Original:\\n\", paragraph)\n",
        "print(\"\\nCleaned:\\n\", cleaned_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install spacy\n",
        "#!python -m spacy download en_core_web_sm\n",
        "#!pip install googletrans==4.0.0-rc1\n"
      ],
      "metadata": {
        "id": "0-FbF5m1uFi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from googletrans import Translator\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Translator\n",
        "translator = Translator()\n"
      ],
      "metadata": {
        "id": "L2J_q-Rwt5kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "OsO9IyN-uV_e",
        "outputId": "580c9f4f-c795-488f-a645-4ca817c35257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hey yash kya haal hai long time no see kal raat i watched avengers endgame mast movie thi btw elon musk announced a new tesla model california 2025 main soch raha tha ki hum log coffee pe milte jab bhi free ho lemme know ok friendsforever'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(cleaned_text)\n",
        "\n",
        "print(\"Named Entities Detected:\\n\")\n",
        "if doc.ents:\n",
        "    for ent in doc.ents:\n",
        "        print(f\"{ent.text:<20} | {ent.label_}\")\n",
        "else:\n",
        "    print(\"No named entities found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXIGZvv-ulFT",
        "outputId": "cc93ad4e-e09b-4326-df9e-9e773838e6e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities Detected:\n",
            "\n",
            "hai                  | GPE\n",
            "california           | GPE\n",
            "2025                 | DATE\n",
            "tha ki hum log coffee | PERSON\n",
            "ho lemme             | PERSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate to Hindi (Devanagari)\n",
        "result_hi = translator.translate(paragraph, src=\"auto\", dest=\"hi\")\n",
        "print(\"\\nüëâ Hindi Translation:\\n\", result_hi.text)\n",
        "\n",
        "# Translate to English (proper English from Hinglish/mixed text)\n",
        "result_en = translator.translate(cleaned_text, src=\"auto\", dest=\"en\")\n",
        "print(\"\\nüëâ English Translation:\\n\", result_en.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--YaRlN9upXV",
        "outputId": "6eedad91-c7b4-4181-b046-ea30fd84711f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üëâ Hindi Translation:\n",
            " Hey Yash üòÉ! Kya haal hai? Long time no see‚Ä¶\n",
            "Kal raat I watched Avengers: Endgame üé¨üî• ‚Äì mast movie thi!\n",
            "BTW, Elon Musk üöÄ announced a new Tesla model @ California (2025).\n",
            "Main soch raha tha ki hum log ‚òï coffee pe milte jab bhi free ho‚Ä¶\n",
            "Lemme know, ok? üëç #FriendsForever ‚ù§Ô∏è\n",
            "\n",
            "üëâ English Translation:\n",
            " Hey yash kya haal hai long time no see kal raat i watched avengers endgame Mast Movie Thi Btw Elon Musk Announced A New Tesla Model California 2025 Mainia 2025 Mainia 2025 Mainia 2025 Mainia Soch Raha Tha Ki Hum Log Cofee Tha Ki Hum Log Cofeejab bhi free ho lemme know ok friendsforever\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wcddBGP8usUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "848135d8"
      },
      "source": [
        "# Task\n",
        "Translate the English parts of the text to Hindi, keeping the original Hindi parts as they are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c04fd959"
      },
      "source": [
        "## Language detection\n",
        "\n",
        "### Subtask:\n",
        "Identify the language of different parts of the cleaned text (English and Hindi).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e68ebb4"
      },
      "source": [
        "**Reasoning**:\n",
        "To identify the language of different parts of the text, I need to iterate through the words or phrases in the cleaned text and use the translator to detect the language of each part. I will split the cleaned text by spaces for this purpose.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3274e344",
        "outputId": "3eb13951-33da-4931-e89c-3e5466018b06"
      },
      "source": [
        "detected_languages = []\n",
        "words = cleaned_text.split()\n",
        "for word in words:\n",
        "    try:\n",
        "        lang = translator.detect(word).lang\n",
        "        detected_languages.append((word, lang))\n",
        "    except Exception as e:\n",
        "        detected_languages.append((word, f\"Error: {e}\"))\n",
        "\n",
        "for word, lang in detected_languages:\n",
        "    print(f\"'{word}': {lang}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'hey': en\n",
            "'yash': en\n",
            "'kya': lg\n",
            "'haal': af\n",
            "'hai': vi\n",
            "'long': en\n",
            "'time': en\n",
            "'no': en\n",
            "'see': en\n",
            "'kal': en\n",
            "'raat': hi\n",
            "'i': en\n",
            "'watched': en\n",
            "'avengers': en\n",
            "'endgame': en\n",
            "'mast': en\n",
            "'movie': en\n",
            "'thi': en\n",
            "'btw': en\n",
            "'elon': en\n",
            "'musk': en\n",
            "'announced': en\n",
            "'a': en\n",
            "'new': en\n",
            "'tesla': en\n",
            "'model': en\n",
            "'california': en\n",
            "'2025': en\n",
            "'main': en\n",
            "'soch': uz\n",
            "'raha': mg\n",
            "'tha': en\n",
            "'ki': mi\n",
            "'hum': en\n",
            "'log': en\n",
            "'coffee': en\n",
            "'pe': en\n",
            "'milte': hi\n",
            "'jab': en\n",
            "'bhi': gd\n",
            "'free': en\n",
            "'ho': st\n",
            "'lemme': en\n",
            "'know': en\n",
            "'ok': en\n",
            "'friendsforever': en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fc94fa5"
      },
      "source": [
        "## Conditional translation\n",
        "\n",
        "### Subtask:\n",
        "Translate only the non-Hindi parts of the text to Hindi.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e567c03"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the detected languages and translate non-Hindi words to Hindi while keeping Hindi words as they are, handling potential translation errors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "077330e4",
        "outputId": "8b1a97e9-c2de-4e49-a6ca-e8ee27553764"
      },
      "source": [
        "translated_words = []\n",
        "for word, lang in detected_languages:\n",
        "    if lang != 'hi':\n",
        "        try:\n",
        "            translation = translator.translate(word, dest='hi')\n",
        "            translated_words.append(translation.text)\n",
        "        except Exception as e:\n",
        "            print(f\"Translation failed for '{word}': {e}\")\n",
        "            translated_words.append(word) # Keep original word if translation fails\n",
        "    else:\n",
        "        translated_words.append(word)\n",
        "\n",
        "translated_text = \" \".join(translated_words)\n",
        "print(\"Translated Text (non-Hindi parts translated):\\n\", translated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated Text (non-Hindi parts translated):\n",
            " ‡§Ö‡§∞‡•á ‡§Ø‡§∂ ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§ö‡•Å‡§®‡§®‡§æ ‡§¶‡•ã ‡§≤‡§Ç‡§¨‡§æ ‡§∏‡§Æ‡§Ø ‡§®‡§π‡•Ä‡§Ç ‡§¶‡•á‡§ñ‡§®‡§æ ‡§ï‡§≤ raat ‡§Æ‡•à‡§Ç ‡§¶‡•á‡§ñ‡§æ ‡§è‡§µ‡•á‡§Ç‡§ú‡§∞‡•ç‡§∏ ‡§è‡§Ç‡§°‡§ó‡•á‡§Æ ‡§Æ‡§∏‡•ç‡§§ ‡§ö‡§≤‡§ö‡§ø‡§§‡•ç‡§∞ ‡§•‡•Ä ‡§¨‡•Ä‡§ü‡•Ä‡§°‡§¨‡•ç‡§≤‡•ç‡§Ø‡•Ç ELON ‡§ï‡§∏‡•ç‡§§‡•Ç‡§∞‡•Ä ‡§ï‡•Ä ‡§ò‡•ã‡§∑‡§£‡§æ ‡§ï‡•Ä ‡§è ‡§®‡§Ø‡§æ ‡§ü‡•á‡§∏‡•ç‡§≤‡§æ ‡§®‡§Æ‡•Ç‡§®‡§æ ‡§ï‡•à‡§≤‡§ø‡§´‡•ã‡§∞‡•ç‡§®‡§ø‡§Ø‡§æ 2025 ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§¨‡§æ‡§≤ ‡§Ö‡§ó‡§∞ ‡§•‡§æ ‡§ï‡•ã ‡§ó‡•Å‡§Ç‡§ú‡§® ‡§≤‡§ï‡§°‡§º‡•Ä ‡§ï‡§æ ‡§≤‡§ü‡•ç‡§†‡§æ ‡§ï‡•â‡§´‡•Ä ‡§™‡•Ä‡§à milte ‡§™‡•ç‡§∞‡§π‡§æ‡§∞ ‡§π‡•ã‡§®‡§æ ‡§Æ‡•Å‡§ï‡•ç‡§§ ‡§ï‡•ã ‡§≤‡•á‡§Æ‡•ç‡§Æ‡•á ‡§ú‡§æ‡§®‡§®‡§æ ‡§†‡•Ä‡§ï ‡§π‡•à ‡§π‡§Æ‡•á‡§∂‡§æ ‡§Æ‡§ø‡§§‡•ç‡§∞ ‡§∞‡§π‡•á‡§Ç‡§ó‡•á\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dda1d0f"
      },
      "source": [
        "## Combine translated parts\n",
        "\n",
        "### Subtask:\n",
        "Combine the translated Hindi parts with the original Hindi parts to form a complete translated sentence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22838866"
      },
      "source": [
        "## Display translation\n",
        "\n",
        "### Subtask:\n",
        "Display the final Hindi translation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e8eed08"
      },
      "source": [
        "**Reasoning**:\n",
        "Print the variable `translated_text` to display the final Hindi translation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f510463",
        "outputId": "f9961ca3-ba90-46e9-b444-a3a8da0513e9"
      },
      "source": [
        "print(translated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‡§Ö‡§∞‡•á ‡§Ø‡§∂ ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§ö‡•Å‡§®‡§®‡§æ ‡§¶‡•ã ‡§≤‡§Ç‡§¨‡§æ ‡§∏‡§Æ‡§Ø ‡§®‡§π‡•Ä‡§Ç ‡§¶‡•á‡§ñ‡§®‡§æ ‡§ï‡§≤ raat ‡§Æ‡•à‡§Ç ‡§¶‡•á‡§ñ‡§æ ‡§è‡§µ‡•á‡§Ç‡§ú‡§∞‡•ç‡§∏ ‡§è‡§Ç‡§°‡§ó‡•á‡§Æ ‡§Æ‡§∏‡•ç‡§§ ‡§ö‡§≤‡§ö‡§ø‡§§‡•ç‡§∞ ‡§•‡•Ä ‡§¨‡•Ä‡§ü‡•Ä‡§°‡§¨‡•ç‡§≤‡•ç‡§Ø‡•Ç ELON ‡§ï‡§∏‡•ç‡§§‡•Ç‡§∞‡•Ä ‡§ï‡•Ä ‡§ò‡•ã‡§∑‡§£‡§æ ‡§ï‡•Ä ‡§è ‡§®‡§Ø‡§æ ‡§ü‡•á‡§∏‡•ç‡§≤‡§æ ‡§®‡§Æ‡•Ç‡§®‡§æ ‡§ï‡•à‡§≤‡§ø‡§´‡•ã‡§∞‡•ç‡§®‡§ø‡§Ø‡§æ 2025 ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§¨‡§æ‡§≤ ‡§Ö‡§ó‡§∞ ‡§•‡§æ ‡§ï‡•ã ‡§ó‡•Å‡§Ç‡§ú‡§® ‡§≤‡§ï‡§°‡§º‡•Ä ‡§ï‡§æ ‡§≤‡§ü‡•ç‡§†‡§æ ‡§ï‡•â‡§´‡•Ä ‡§™‡•Ä‡§à milte ‡§™‡•ç‡§∞‡§π‡§æ‡§∞ ‡§π‡•ã‡§®‡§æ ‡§Æ‡•Å‡§ï‡•ç‡§§ ‡§ï‡•ã ‡§≤‡•á‡§Æ‡•ç‡§Æ‡•á ‡§ú‡§æ‡§®‡§®‡§æ ‡§†‡•Ä‡§ï ‡§π‡•à ‡§π‡§Æ‡•á‡§∂‡§æ ‡§Æ‡§ø‡§§‡•ç‡§∞ ‡§∞‡§π‡•á‡§Ç‡§ó‡•á\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68cfe276"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* The initial attempt to detect language word by word using `googletrans` resulted in inaccurate language identification, particularly for Hindi words and short phrases.\n",
        "* The subtask of identifying the language of different parts of the text could not be successfully completed due to the limitations of the language detection method.\n",
        "* The non-Hindi parts of the text were successfully translated to Hindi.\n",
        "* The task of combining translated and original parts was deemed unnecessary as it was handled in a previous step.\n",
        "* The final translated text was successfully displayed.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* A more robust language detection method is needed for accurately identifying English and Hindi parts in mixed-language text.\n",
        "* Consider using a library or API that supports sentence-level or phrase-level language detection for better accuracy in mixed-language scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "from googletrans import Translator\n",
        "from spacy import displacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize translator\n",
        "translator = Translator()\n"
      ],
      "metadata": {
        "id": "HeywF-CUwYLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"\n",
        "Hey Yash üòÉ! Kya haal hai? Long time no see‚Ä¶\n",
        "Kal raat I watched Avengers: Endgame üé¨üî• ‚Äì mast movie thi!\n",
        "BTW, Elon Musk üöÄ announced a new Tesla model @ California (2025).\n",
        "Main soch raha tha ki hum log ‚òï coffee pe milte jab bhi free ho‚Ä¶\n",
        "Lemme know, ok? üëç #FriendsForever ‚ù§Ô∏è\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "8ido23Y3wZAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def light_preprocess(text):\n",
        "    # Remove unwanted special characters (keeping punctuation and emojis)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?@#‚ù§Ô∏èüî•üé¨‚òïüöÄüòÉüëç]', '', text)\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "cleaned_text = light_preprocess(paragraph)\n",
        "print(\"Cleaned Text:\\n\", cleaned_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NudctIaswbgN",
        "outputId": "5729c52c-ba82-4661-f5a3-59d9e41cd855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Text:\n",
            " Hey Yash üòÉ! Kya haal hai? Long time no see Kal raat I watched Avengers Endgame üé¨üî• mast movie thi! BTW, Elon Musk üöÄ announced a new Tesla model @ California 2025. Main soch raha tha ki hum log ‚òï coffee pe milte jab bhi free ho Lemme know, ok? üëç #FriendsForever ‚ù§Ô∏è\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple split by punctuation marks\n",
        "sentences = re.split(r'[.!?]\\s*', cleaned_text)\n",
        "# Remove empty strings\n",
        "sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "print(\"Sentences:\\n\", sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERckcSsHwdfk",
        "outputId": "96769661-ad59-4993-e237-0c953d808876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            " ['Hey Yash üòÉ', 'Kya haal hai', 'Long time no see Kal raat I watched Avengers Endgame üé¨üî• mast movie thi', 'BTW, Elon Musk üöÄ announced a new Tesla model @ California 2025', 'Main soch raha tha ki hum log ‚òï coffee pe milte jab bhi free ho Lemme know, ok', 'üëç #FriendsForever ‚ù§Ô∏è']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for s in sentences:\n",
        "    # 1Ô∏è‚É£ NER\n",
        "    doc = nlp(s)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    # 2Ô∏è‚É£ Translate to Hindi\n",
        "    translated_hi = translator.translate(s, src='auto', dest='hi').text\n",
        "\n",
        "    # 3Ô∏è‚É£ Translate to English (proper English from Hinglish)\n",
        "    translated_en = translator.translate(s, src='auto', dest='en').text\n",
        "\n",
        "    # Store results\n",
        "    results.append({\n",
        "        'original': s,\n",
        "        'entities': entities,\n",
        "        'hindi': translated_hi,\n",
        "        'english': translated_en\n",
        "    })\n",
        "\n",
        "# Display results\n",
        "for i, res in enumerate(results, 1):\n",
        "    print(f\"\\n--- Sentence {i} ---\")\n",
        "    print(\"Original:\", res['original'])\n",
        "    print(\"Entities:\", res['entities'] if res['entities'] else \"No entities detected\")\n",
        "    print(\"Hindi Translation:\", res['hindi'])\n",
        "    print(\"English Translation:\", res['english'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jfx6lcQ7wf5-",
        "outputId": "21a6fc77-a565-4d01-b655-7ea8887d8f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sentence 1 ---\n",
            "Original: Hey Yash üòÉ\n",
            "Entities: No entities detected\n",
            "Hindi Translation: ‡§π‡•á ‡§Ø‡§∂ üòÉ\n",
            "English Translation: Hey Yash üòÉ\n",
            "\n",
            "--- Sentence 2 ---\n",
            "Original: Kya haal hai\n",
            "Entities: [('hai', 'GPE')]\n",
            "Hindi Translation: Kya haal hai\n",
            "English Translation: How are you\n",
            "\n",
            "--- Sentence 3 ---\n",
            "Original: Long time no see Kal raat I watched Avengers Endgame üé¨üî• mast movie thi\n",
            "Entities: [('Kal', 'PERSON')]\n",
            "Hindi Translation: ‡§≤‡§Ç‡§¨‡•á ‡§∏‡§Æ‡§Ø ‡§∏‡•á ‡§®‡§π‡•Ä‡§Ç ‡§¶‡•á‡§ñ‡§æ ‡§ó‡§Ø‡§æ ‡§ï‡§≤ ‡§∞‡§æ‡§ü ‡§Æ‡•à‡§Ç‡§®‡•á ‡§è‡§µ‡•á‡§Ç‡§ú‡§∞‡•ç‡§∏ ‡§è‡§Ç‡§°‡§ó‡•á‡§Æ üé¨üî• ‡§Æ‡§æ‡§∏‡•ç‡§ü ‡§Æ‡•Ç‡§µ‡•Ä ‡§•‡•Ä\n",
            "English Translation: Long time no see Kal raat I watched Avengers Endgame üé¨üî• mast movie thi\n",
            "\n",
            "--- Sentence 4 ---\n",
            "Original: BTW, Elon Musk üöÄ announced a new Tesla model @ California 2025\n",
            "Entities: [('Elon Musk üöÄ', 'PERSON'), ('Tesla', 'ORG'), ('California 2025', 'EVENT')]\n",
            "Hindi Translation: BTW, ‡§è‡§≤‡•ã‡§® ‡§Æ‡§∏‡•ç‡§ï üöÄ ‡§®‡•á ‡§è‡§ï ‡§®‡§è ‡§ü‡•á‡§∏‡•ç‡§≤‡§æ ‡§Æ‡•â‡§°‡§≤ @ ‡§ï‡•à‡§≤‡§ø‡§´‡•ã‡§∞‡•ç‡§®‡§ø‡§Ø‡§æ 2025 ‡§ï‡•Ä ‡§ò‡•ã‡§∑‡§£‡§æ ‡§ï‡•Ä\n",
            "English Translation: BTW, Elon Musk üöÄ announced a new Tesla model @ California 2025\n",
            "\n",
            "--- Sentence 5 ---\n",
            "Original: Main soch raha tha ki hum log ‚òï coffee pe milte jab bhi free ho Lemme know, ok\n",
            "Entities: [('tha ki hum log', 'PERSON'), ('ho Lemme', 'PERSON')]\n",
            "Hindi Translation: Main soch raha tha ki hum log ‚òï coffee pe milte jab bhi free ho Lemme know, ok\n",
            "English Translation: Main Soch Raha Tha Ki Hum Log ‚òï Coffee Pe Milte Jab Bhi Free Ho Lemme Know, Ok\n",
            "\n",
            "--- Sentence 6 ---\n",
            "Original: üëç #FriendsForever ‚ù§Ô∏è\n",
            "Entities: [('FriendsForever', 'PERSON')]\n",
            "Hindi Translation: üëç #FriendSforever ‚ù§\n",
            "English Translation: üëç #FriendsForever ‚ù§Ô∏è\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c_otbwiZxOxE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}