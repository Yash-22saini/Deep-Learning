{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a94cce3b-2195-45d6-b886-806035b3a4ed",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "## Activation functions are mathematical functions applied to the outputs of individual neurons in a neural network. They introduce non-linearity into the network, allowing it to learn and approximate complex relationships between inputs and outputs. \n",
    "\n",
    "## some commonly used activation functions in deep learning:\n",
    "\n",
    "(1).Sigmoid function (Logistic function): It maps the input to a value between 0 and 1. It was widely used in the past but is now\n",
    "less popular due to some drawbacks such as vanishing gradients.\n",
    "\n",
    "(2).Hyperbolic tangent function (tanh): Similar to the sigmoid function, but it maps the input to a value between -1 and 1. It is\n",
    "still used in some cases, but it also suffers from vanishing gradients.\n",
    "\n",
    "(3).Rectified Linear Unit (ReLU): This function sets all negative values to zero and keeps positive values unchanged. It is the\n",
    "most popular activation function in deep learning due to its simplicity and effectiveness in training deep neural networks.\n",
    "\n",
    "(4).Leaky ReLU: This function is similar to ReLU but allows a small negative slope for negative input values. It helps mitigate\n",
    "the \"dying ReLU\" problem where some neurons can become permanently inactive during training.\n",
    "\n",
    "(5).Parametric ReLU (PReLU): PReLU is a generalization of ReLU that introduces a learnable parameter to determine the\n",
    "slope of negative input values. It offers more flexibility and can improve model performance.\n",
    "\n",
    "(6).Exponential Linear Unit (ELU): ELU is a variation of ReLU that allows negative values with a smooth exponential decay. Ithelps alleviate the dying ReLU problem and can produce more robust models.\n",
    "\n",
    "(7).Softmax: Softmax is commonly used in the output layer of a neural network for multi-class classification problems. It\n",
    "normalizes the output values to represent probabilities, ensuring that the sum of all probabilities is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb3004e-df38-4f00-b7b5-67eeb9cf6065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
