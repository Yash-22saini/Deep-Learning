{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc5de93c",
   "metadata": {},
   "source": [
    "\n",
    "# TRANSFER LEARNING – NOTES\n",
    "\n",
    "## 1. What is Transfer Learning?\n",
    "Transfer Learning is a machine learning technique where a model trained on one task is reused for another related task.  \n",
    "It helps use the knowledge gained from one domain to improve learning in another.\n",
    "\n",
    "**Goal:** Reuse a pre-trained model instead of training from scratch.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why Use Transfer Learning?\n",
    "- Saves training time and resources  \n",
    "- Requires less labeled data  \n",
    "- Improves model performance  \n",
    "- Reduces overfitting  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. When to Use\n",
    "**Use when:**\n",
    "- Limited data for the target task  \n",
    "- Tasks are related  \n",
    "- Base model is trained on a large dataset  \n",
    "\n",
    "**Avoid when:**\n",
    "- Target task is very different  \n",
    "- Enough data is available to train from scratch  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Types of Transfer Learning\n",
    "1. **Feature Extraction:**  \n",
    "   Use the pre-trained model as a fixed feature extractor; only train new classifier layers.  \n",
    "\n",
    "2. **Fine-Tuning:**  \n",
    "   Unfreeze some layers of the pre-trained model and retrain them on new data.  \n",
    "\n",
    "3. **Domain Adaptation:**  \n",
    "   Adapt knowledge from one domain to another (e.g., photos → sketches).  \n",
    "\n",
    "4. **Multi-Task Learning:**  \n",
    "   Train multiple related tasks simultaneously using shared features.  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. Transfer Learning Workflow\n",
    "1. Choose a pre-trained model (VGG, ResNet, BERT, etc.)  \n",
    "2. Load pre-trained weights  \n",
    "3. Freeze initial layers  \n",
    "4. Add new layers for the target task  \n",
    "5. Train on your dataset  \n",
    "6. Optionally unfreeze some layers for fine-tuning  \n",
    "\n",
    "---\n",
    "\n",
    "## 6. Example – Image Classification (CNN)\n",
    "```python\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_data, train_labels, epochs=5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Example – NLP (BERT)\n",
    "```python\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_texts, train_labels, epochs=3)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Popular Pre-trained Models\n",
    "\n",
    "| Domain | Model Examples | Dataset |\n",
    "|--------|----------------|----------|\n",
    "| Computer Vision | VGG16, ResNet, Inception, MobileNet | ImageNet |\n",
    "| NLP | BERT, GPT, RoBERTa, T5 | Wikipedia, BooksCorpus |\n",
    "| Speech | Wav2Vec, DeepSpeech | LibriSpeech |\n",
    "| Multimodal | CLIP, DALL·E | Image-text pairs |\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Advantages\n",
    "- Faster convergence  \n",
    "- High accuracy  \n",
    "- Works with small datasets  \n",
    "- Reduces overfitting  \n",
    "\n",
    "---\n",
    "\n",
    "## 10. Disadvantages\n",
    "- Negative transfer (poor performance if domains differ)  \n",
    "- May overfit if fine-tuned incorrectly  \n",
    "- High memory usage for large models  \n",
    "\n",
    "---\n",
    "\n",
    "## 11. Real-world Applications\n",
    "- Image classification (e.g., medical scans, animals)  \n",
    "- Sentiment analysis  \n",
    "- Object detection  \n",
    "- Speech recognition  \n",
    "- Text summarization and translation  \n",
    "\n",
    "---\n",
    "\n",
    "## 12. Key Terms\n",
    "- **Base Model:** Pre-trained model used as starting point  \n",
    "- **Fine-tuning:** Retraining part of the model on new data  \n",
    "- **Freezing:** Keeping certain layers fixed during training  \n",
    "- **Feature Extractor:** Using pre-trained layers for feature generation  \n",
    "\n",
    "---\n",
    "\n",
    "## 13. Important Interview Questions\n",
    "1. What is transfer learning?  \n",
    "2. Why is it used?  \n",
    "3. Difference between feature extraction and fine-tuning.  \n",
    "4. When does transfer learning fail?  \n",
    "5. Examples of pre-trained models in vision and NLP.  \n",
    "6. What is negative transfer?  \n",
    "7. Why freeze layers?  \n",
    "8. How is BERT an example of transfer learning?  \n",
    "9. Explain how to fine-tune a CNN.  \n",
    "10. Give real-world uses of transfer learning.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
